---
title: "INFSCI 2595 Final Project"
subtitle: "Part 3D1, Classification - Resampling - Advanced Models"
author: "Zhenyu Li"
date: '2022-04-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_clas <- df %>% select(-c(rowid,response)) # remove unused columns
```


## Advanced Models

Train the following advanced models for the classification.

- Neural network
- Random forest
- Gradient boosted tree
- Multivariate Additive Regression Splines
- Support Vector Machine

### Models

**The preprocessing steps.**  

```{r}
bp_3D <- recipe(outcome ~ ., data=df_clas) %>%
  step_dummy(all_nominal_predictors())
bp_3D_norm <- bp_3D %>%
  step_normalize(all_numeric_predictors())
```

**Model specifications.**  
  
```{r}
nnet_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = 2000) %>%
  set_engine("nnet", MaxNWts = 2000, trace=FALSE) %>%
  set_mode("classification")

rf_spec <-
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
   set_engine("ranger") %>%
   set_mode("classification")

xgb_spec <-
   boost_tree(tree_depth = tune(), learn_rate = tune(),
              trees = tune(), mtry = tune(), sample_size = tune()) %>%
   set_engine("xgboost") %>%
   set_mode("classification")

mars_spec <-
   mars(prod_degree = tune()) %>%
   set_engine("earth") %>%
   set_mode("classification")

svm_spec <-
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
   set_engine("kernlab") %>%
   set_mode("classification")
```

### Workflows

**Create two workflows based on whether input normalization preprocessing is required.**  

```{r normalized}
adv_wset_norm_cl <-
  workflow_set(
    preproc = list(bp_3D_norm),
    models = list(
      SVM = svm_spec,
      NNet = nnet_spec
    )
  )
```

```{r not-normalized}
adv_wset_no_preproc_cl <-
  workflow_set(
    preproc = list(bp_3D),
    models = list(
      MARS = mars_spec,
      RF = rf_spec,
      XGB = xgb_spec
    )
  )
```

**Combine the workflows into one for training.**  

```{r}
adv_wset_all_cl <- bind_rows(adv_wset_no_preproc_cl, adv_wset_norm_cl) %>%
  mutate(wflow_id = gsub("(recipe_)","",wflow_id))
```

----

## Training

### Setup

**Resampling setup.**  

```{r resampling}
set.seed(1324)
cv_folds <- vfold_cv(df_clas, v = 5, repeats = 5)
my_metrics_cl <- metric_set(accuracy, roc_auc, mn_log_loss)
```

### Execute

**Perform model tuning with default grid of 50 combinations for tuning parameters.**  

Note: to speed up knit, these two code blocks are set `eval=FALSE`. They can be manually executed in the rmarkdown file.  

```{r execute, eval=FALSE}
# create cluster to allow parallel computing
if (parallel::detectCores(logical=FALSE) > 3) {
  library(doParallel)
  num_cores <- parallel::detectCores(logical=FALSE)
  cl <- makePSOCKcluster(num_cores - 2)
  registerDoParallel(cl)
}
# start tuning
tune_3D_adv <- adv_wset_all_cl %>%
  workflow_map(
    fn = "tune_grid",
    resamples = cv_folds,
    metrics = my_metrics_cl,
    grid = 20,
    control = control_grid(
      save_pred = FALSE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = TRUE
  )
# close the cluster
stopCluster(cl)
registerDoSEQ()
```

```{r save_result, eval=FALSE}
readr::write_rds(tune_3D_adv,"./models/tune_3D_adv")
```

----

## Conclusions

```{r}
tune_3D_adv <- readr::read_rds("./models/tune_3D_adv")
names(tune_3D_adv$result) <- tune_3D_adv$wflow_id
```

### Performance

**Examine metrics across all models trained.**

```{r visualize grid, out.width=1000, fig.width=10, fig.asp=0.8, message=FALSE, warning=FALSE}
tune_3D_adv %>%
  autoplot(
    rank_metric = "roc_auc",
    metric = c("accuracy","roc_auc","mn_log_loss","p"),
    select_best = F) +
  facet_wrap(~.metric, scale = "free_y", ncol = 1)
```

- The result is ranked by ROC AUC, and we can see the SVM models seem to have top ranked performances except accuracy.  

**Compare the model performance metrics for the best tuning parameters for each model type.**  

```{r visualize best, out.width=800, fig.width=8, fig.asp=0.4, message=FALSE, warning=FALSE}
tune_3D_adv %>%
  autoplot(
    rank_metric = "accuracy",
    metric = c("accuracy","roc_auc","mn_log_loss"),
    select_best = T) +
  geom_text(
    aes(y = mean, label = wflow_id),
    nudge_x = 0.2, angle = 90) +
  theme(legend.position = "none")
```


- NNet though has great accuracy and ROC, the mean log loss is quite high compared to the other models.  
- Judging from the three metrics, XGB has best overall performances though RF is also not significantly different.  
- Arbitrarily determine XGB as best model.  

### Best Model

**Extract the best tuning parameters from the best model.**  

```{r}
adv_best_wf_cl <- "XGB"

adv_best_roc_params <-
  tune_3D_adv$result[[adv_best_wf_cl]] %>%
  select_best(metric = "accuracy")
adv_best_roc_params %>% select(-starts_with("."))
```

**Finalize the workflow for the best model and retrain with resampling to save predictions.** 

```{r}
mod_3D_adv_best_wflow <-
  adv_wset_all_cl %>%
  extract_workflow(adv_best_wf_cl) %>%
  finalize_workflow(parameters = adv_best_roc_params)

mod_3D_adv_best_resample <-
  mod_3D_adv_best_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = my_metrics_cl,
    control = control_resamples(save_pred = T)
  )
```

```{r}
save_models(mget(ls(pattern = "^mod_3D_adv")))
```


----