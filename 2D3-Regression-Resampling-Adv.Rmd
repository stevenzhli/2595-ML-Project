---
title: "INFSCI 2595 Final Project"
subtitle: "Part 2D3, Regression - Resampling - Advanced Models"
author: "Zhenyu Li"
date: '2022-04-20'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_regr <- df %>% select(-c(rowid,outcome)) # remove unused columns
```

## Advanced Models

Train the following advanced models.

- Neural network
- Random forest
- Gradient boosted tree
- Multivariate Additive Regression Splines
- Support Vector Machine
- K-Nearest Neighborhood

### Models

**The preprocessing steps.**

```{r}
bp_2D_log <- recipe(response ~ ., data=df_regr) %>% 
  step_log(all_outcomes()) %>% 
  step_dummy(all_nominal_predictors())
bp_2D_norm <- bp_2D_log %>% 
  step_normalize(all_numeric_predictors())
```

**Model specifications.**
  
```{r}
nnet_spec <- 
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
  set_engine("nnet", MaxNWts = 2000, trace=FALSE) %>% 
  set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), 
              trees = tune(), mtry = tune(), sample_size = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")

mars_spec <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("regression")

svm_spec <- 
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")
```

### Workflows

**Create two workflows based on whether input normalization preprocessing is required.**  

```{r normalized}
adv_wset_norm <- 
  workflow_set(
    preproc = list(bp_2D_norm),
    models = list(
      SVM = svm_spec,
      NNet = nnet_spec
    )
  )
```

```{r not-normalized}
adv_wset_no_preproc <- 
  workflow_set(
    preproc = list(bp_2D_log),
    models = list(
      MARS = mars_spec,
      RF = rf_spec,
      XGB = xgb_spec 
    )
  )
```

**Combine the workflows into one for training. **  

```{r}
adv_wset_all <- bind_rows(adv_wset_no_preproc, adv_wset_norm) %>% 
  mutate(wflow_id = gsub("(recipe_)","",wflow_id))
```

## Tuning

### Setup

```{r resampling}
set.seed(2356)
adv_folds <- vfold_cv(df_regr, v = 5, repeats = 3) #DEBUG
my_metrics <- metric_set(rmse, mae, rsq)
```

### Execute

**Perform model tuning with default grid and 10 levels at each tuning parameter.**  

```{r message=FALSE, warning=FALSE}
adv_tune_2D <- adv_wset_all %>% 
  workflow_map(
    fn = "tune_grid",
    resamples = adv_folds,
    metrics = my_metrics,
    grid = 10,
    control = control_grid(
      save_pred = FALSE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = TRUE
  )
```

```{r save result}
readr::write_rds(adv_tune_2D,"./models/adv_tune_2D")
```

## Conclusions

```{r}
adv_tune_2D <- readr::read_rds("./models/adv_tune_2D")
```

### Performance

**Visualize the model performance metrics across the models tuned.**  

```{r visualize grid, out.width=600, fig.width=8, fig.asp=0.6, message=FALSE, warning=FALSE}
names(adv_tune_2D$result) <- adv_tune_2D$wflow_id
adv_tune_2D %>% 
  autoplot( 
    rank_metric = "mae",
    metric = c("mae","rmse","rsq"),
    select_best = T) + 
  geom_text(
    aes(y = mean, label = wflow_id), 
    nudge_x = 0.2, angle = 90) + 
  theme(legend.position = "none")
```

- Judging from the visualization
  - SVM method has best performance for all three metrics. 
  
**Print out the value of model metrics for quick comparison. **  

```{r}
adv_tune_2D %>% rank_results(rank_metric = "mae", select_best = T)
```

### Best Model

**Select the best tuning parameters based on the one-standard-error rule.**  

```{r}
adv_best_mae_params <- 
  adv_tune_2D$result$SVM %>% select_by_one_std_err(rbf_sigma, metric = "mae")
adv_best_mae_params %>% select(-starts_with("."))
```

**Finalize the workflow for the best model and retrain to save predictions.** 

```{r}
adv_best_wflow_mae <- "SVM"

adv_best_wflow <- adv_wset_all %>% 
  extract_workflow(adv_best_wflow_mae) %>% 
  finalize_workflow(
    parameters = adv_best_mae_params
  )

adv_best_2D <- adv_best_wflow %>% 
  fit_resamples(
    adv_folds,
    resamples = adv_folds,
    metrics = my_metrics,
    control = control_resamples(save_pred = T)
  )
```


```{r}
save_models(list(adv_best_2D = adv_best_2D))
```

----
