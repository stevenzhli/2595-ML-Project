---
subtitle: "Part 2C, Regression - Model Predictions"
author: "Zhenyu Li"
date: '2022-04-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(bayesplot)
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_preproc <- df %>% 
  mutate_if(is.character,as.factor) %>% 
  select(-c(rowid,outcome)) %>% 
  mutate(
    response = log(response),
    across(starts_with("x"), scale)
  )
my_inputs <- setdiff(names(df_preproc),"response")
```

## Model Selection

- Select two models from 2A/2B for prediction.   
  - Decided to use Bayesian models from 2B for prediction.  

- Choose two with similar posterior Ïƒ but different features
  - One with `region` interacting inputs with non-linear basis (2 dof natural spline) `mod_2B2`. 
  - One with `region` interacting with same set of inputs but without the non-linear basis `mod_2B3`. 
    - This is the best model identified by WAIC.  

```{r}
mod_2B2 <- readr::read_rds("models/mod_2B2")
mod_2B3 <- readr::read_rds("models/mod_2B3")
```

----

## Prediction

### Input Selection

- Visualize the significant regression coefficients for both models.  

```{r coefplot mod_2B2}
mod_2B2 %>% as_tibble() %>% 
  select(all_of(
    mod_2B2 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```
- Among these significant coefficients:   
  - The linear predictors have much less magnitude and uncertainties.  
  - Interaction between `region` to non-linear predictors have greater uncertainty, but also magnitudes.  

```{r coefplot mod_2B3}
mod_2B3 %>% as_tibble() %>% 
  select(all_of(
    mod_2B3 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

- All of these linear predictor coefficients are quite small compared to the nonlinear predictors from `mod_2B2`. 

- Choose predictors to use based on both 
  - The magnitude of the estimated coefficient, and 
  - If two coefficients show significant difference when compared on the coefficient plot. 
  
- Selected
  1. xb_07, xb_04 (linear, linear)
  2. xb_07, customer (linear, category)
  3. xa_08, xb_07, region (nonlinear, linear, category)
  4. xw_01, xb_05, region (nonlinear, nonlinear, category)

- Note: since all continuous inputs are standardized to train the models, the prediction test grid is just created at the same standardized scale using the preprocessed dataset. The predictors thus have the standardized scales, not the actual scale from the original data set. But the trends should remain. 

----

## Predictions

### xb_07, xb_04

```{r mod_2B2, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B2, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- The log-response increases with the linear predictor `xb_07` (each fact).
- The log-response decreases with the linear predictor `xb_04` (horizontal facts, shift trends down).
- There is higher uncertainty on the model behavior (mean trend) when inputs are at extreme values. 

```{r mod_2B3, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B3, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```


### xb_07, customer

```{r mod_2B2, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B2, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```

- `Customer` affects how the linear predictor `xb_07` predicts the log-response.  
  - Customers `A`,`K` tend to have higher response. 
  - Customers `M`,`Q` tend to have lower response. 

```{r mod_2B3, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B3, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```


### xa_08, xb_07, region

```{r mod_2B2, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B2, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Effect of nonlinear predictor `xa_08` depends on the `region`:
  - In region `XX`, its increase slightly decreases log-response;
  - In region `YY`, its increase can greatly decrease log-response;
  - It does not have too much effect for region `ZZ`. 
- The nonlinear predictors have large model uncertainties (mean behavior), especially at extreme predictor values. 
- Increasing the linear predictor `xb_07` slightly increase the response (horizontal facets).
  - Though the magnitude of the effect is overshadowed by the nonlinear predictor. 

```{r mod_2B3, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B3, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Same trends as with `mod_2B2`, but all linear.
- Less model uncertainty (grey ribbon) compared to `mod_2B2` with non-linear predictors. 

### xb_05, xw_01, region 

```{r mod_2B2, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B2, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- Increasing nonlinear predictor `xb_05` has different effects based on `region`:
  - Slightly increase the log-response at regions `XX` and `ZZ`
  - Slightly decrease the log-response at region `YY`
- The nonlinear predictor `xw_01` increases response. 

```{r mod_2B3, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B3, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- Same trends as with `mod_2B2`, but linear.
- Less model uncertainty (grey ribbon) compared to `mod_2B2` with non-linear predictors. 

----

## Conclusions

- For the inputs that are visualized, the predictive trends are consistent across the two models.  
- The linear predictor trends are almost identical. 
- The nonlinear predictors from `mod_2B2` allow more range of predicted responses, but also resulted more uncertainty, especially at extreme input values. 

----

  
