---
subtitle: "Part 2C, Regression - Model Predictions"
author: "Zhenyu Li"
date: '2022-04-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(bayesplot)
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_preproc <- df %>% 
  mutate_if(is.character,as.factor) %>% 
  select(-c(rowid,outcome)) %>% 
  mutate(
    response = log(response),
    across(starts_with("x"), scale)
  )
my_inputs <- setdiff(names(df_preproc),"response")
```

## Model Selection

- Select two models from 2A/2B for prediction.   
  - Decided to use Bayesian models from 2B for prediction.  

- Choose two with similar posterior Ïƒ but different features
  - One with `region` interacting inputs with non-linear basis (2 dof natural spline) `mod_2B3_2Ac3`. 
    - This is the best model identified using WAIC using Bayesian.
  - One with only linear additive inputs `mod_2B1_2A3`. 
    - This is the best model identified by BIC in MLE model fitting.  

```{r}
mod_2B1_2A3 <- readr::read_rds("models/mod_2B1_2A3")
mod_2B3_2Ac3 <- readr::read_rds("models/mod_2B3_2Ac3")
```

----

## Prediction

### Input Selection

- Visualize the significant regression coefficients for both models.  

```{r coefplot mod_2B3_2Ac3}
mod_2B3_2Ac3 %>% as_tibble() %>% 
  select(all_of(
    mod_2B3_2Ac3 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```
- Among these significant coefficients:   
  - The linear predictors have much less magnitudes and uncertainties.  
  - Interaction between `region` to non-linear predictors have greater uncertainty, but also magnitudes.  

```{r coefplot mod_2B1_2A3}
mod_2B1_2A3 %>% as_tibble() %>% 
  select(all_of(
    mod_2B1_2A3 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

- All of these linear predictor coefficients are quite small compared to the nonlinear predictors from `mod_2B3_2Ac3`. 

- Choose predictors to use based on both 
  - The magnitude of the estimated coefficient, and 
  - If two coefficients show significant difference when compared on the coefficient plot. 
  
- Selected
  1. xb_07, xb_04 (linear, linear)
  2. xb_07, customer (linear, category)
  3. xa_08, xb_07, region (nonlinear, linear, category)
  4. xw_01, xb_05, region (nonlinear, nonlinear, category)

- Note: since all continuous inputs are standardized to train the models, the prediction test grid is just created at the same standardized scale using the preprocessed dataset. The predictors thus have the standardized scales, not the actual scale from the original data set. But the trends should remain. 

----

## Predictions

### xb_07, xb_04

```{r mod_2B3_2Ac3, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- The log-response increases with the linear predictor `xb_07` (each fact).
- The log-response decreases with the linear predictor `xb_04` (horizontal facts, shift trends down).
- There is higher uncertainty on the model behavior (mean trend) when inputs are at extreme values. 

```{r mod_2B1_2A3, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B1_2A3, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```


### xb_07, customer

```{r mod_2B3_2Ac3, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```

- `Customer` affects how the linear predictor `xb_07` predicts the log-response.  
  - Customers `A`,`K` tend to have higher response. 
  - Customers `M`,`Q` tend to have lower response. 

```{r mod_2B1_2A3, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```


### xa_08, xb_07, region

```{r mod_2B3_2Ac3, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Effect of nonlinear predictor `xa_08` depends on the `region`:
  - In region `XX`, its increase slightly decreases the log-response;
  - In region `YY`, its increase greatly decreases the log-response;
  - It does not have too much effect for region `ZZ`. 
- The nonlinear predictors have large model uncertainties (mean behavior), especially at extreme predictor values. 
  - This may suggest overfit when the inputs are at extremes. 
- Increasing the linear predictor `xb_07` slightly increase the response (horizontal facets).
  - Though the magnitude of the effect is overshadowed by the nonlinear predictor (the coefficients are small). 

```{r mod_2B1_2A3, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Compared to the model with `region` interacting with nonlinear features, this linear additive only model is less flexible and unable to adapt to the trend based on `region` because there is no interaction. 
  - All we get is that region shifts the trend up and down. 
- This resulted higher prediction uncertainty but relatively smaller model uncertainty as compared to the other model.  

### xb_05, xw_01, region 

```{r mod_2B3_2Ac3, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- Increasing nonlinear predictor `xb_05` has different effects based on `region`:
  - Slightly increase the log-response at regions `XX` and `ZZ`
  - Slightly decrease the log-response at region `YY`
- The nonlinear predictor `xw_01` increases response. 

```{r mod_2B1_2A3, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- We see similar effect as with the prior comparisons, that the `3A` model is unable to fit more closely to observations in the specific category, since we did not involve any interaction in this model. 

----

## Conclusions

- For the inputs that are visualized, whether predictive trends are consistent across the two models depends on what we examine.
- The trends from simple additive linear predictors and categorical predictors are almost identical. 

- For inputs that involve interaction:
  - The model with interaction from `region` to continuous inputs is more capable of explaining the observed variation per `region` category, as compared to the model with no interaction. 
  - But this may cause higher risk of overfitting, when the observations are rare: in our data, when the inputs are at extreme values. 

----

  
