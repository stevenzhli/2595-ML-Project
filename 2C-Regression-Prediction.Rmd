---
title: "INFSCI 2595 Final Project"
subtitle: "Part 2C, Regression - Model Predictions"
author: "Zhenyu Li"
date: '2022-04-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(bayesplot)
source("./scripts/utils.R")
```

In this section, I will visualize how the linear models created earlier could predict the `response` based on some selected inputs.  


```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_preproc <- df %>% 
  mutate_if(is.character,as.factor) %>% 
  select(-c(rowid,outcome)) %>% 
  mutate(
    response = log(response),
    across(starts_with("x"), scale)
  )
my_inputs <- setdiff(names(df_preproc),"response")
```

----

## Prediction

### Model Selection

**Select two Bayesian models from 2B for prediction and visualization.**   

- Choose two with similar posterior Ïƒ but different features

- One with `region` interacting inputs with non-linear basis (2 dof natural spline) `mod_2B3_2Ac3`. 
  - This is the best model identified using WAIC using Bayesian.
- One with only linear additive inputs `mod_2B1_2A3`. 
  - This is the best model identified by BIC in MLE model fitting.  

```{r}
mod_2B1_2A3 <- readr::read_rds("models/mod_2B1_2A3")
mod_2B3_2Ac3 <- readr::read_rds("models/mod_2B3_2Ac3")
```

### Input Selection

**Select inputs to visualize, based on model coefficient significance.**  

**First model, coefficient plot**  

```{r mod_2B3_2Ac3}
mod_2B3_2Ac3 %>% as_tibble() %>% 
  select(all_of(
    mod_2B3_2Ac3 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

- Among these significant coefficients:   
  - The linear predictors have much less magnitudes and uncertainties.  
  - Interaction between `region` to non-linear predictors have greater uncertainty, but also magnitudes.  

**Second model, coefficient plot**  

```{r mod_2B1_2A3}
mod_2B1_2A3 %>% as_tibble() %>% 
  select(all_of(
    mod_2B1_2A3 %>% pull_bayes_coefs(signif=T) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

- All of these linear predictor coefficients are quite small compared to the nonlinear predictors from first model `mod_2B3_2Ac3`. 

**Choose predictors to use based on both models. **  

- Selection criterion
  1. Since inputs are normalized, the greater magnitude of the estimated coefficient indicates more impact from this input to the `response`.
  2. If two coefficients show significant difference on the coefplot they worth looking at. 
  3. Examine both continuous and categorical inputs. 

- Selected 4 sets of inputs for visualization
  1. xb_07, xb_04 (linear, linear)
  2. xb_07, customer (linear, category)
  3. xa_08, xb_07, region (nonlinear, linear, category)
  4. xw_01, xb_05, region (nonlinear, nonlinear, category)

- Note: since all continuous inputs are normalized to allow better comparisons, the prediction test grid is just created at the same scale using the preprocessed dataset. The predictors thus have the normalized scales, not the actual scale from the original data set. But the trends should remain. 

----

## Predictions

### Set 1: xb_07, xb_04

**First model**  

```{r mod_2B3.1, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- The log-response increases with the linear predictor `xb_07` (each fact).
- The log-response decreases with the linear predictor `xb_04` (horizontal facts, shift trends down).
- There is higher uncertainty on the model behavior (mean trend) when inputs are at extreme values. 

**Second model**  

```{r mod_2B1.1, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07", "xb_04"))

make_post_pred(mod_2B1_2A3, viz_grid) %>% ggplot(aes(x=xb_07, fill=xb_04)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xb_04, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_04", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- Same trends as with first model.  

### Set 2: xb_07, customer

**First model**  

```{r mod_2B3.2, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```

- `Customer` affects how the linear predictor `xb_07` predicts the log-response.  
  - Customers `A`,`K` tend to have higher response. 
  - Customers `M`,`Q` tend to have lower response. 

**Second model**  

```{r mod_2B1.2, out.width=800, fig.width=8, fig.asp=0.20, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_07"), "customer")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xb_07, fill=customer)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "customer", breaks = NULL, labels = NULL)) +
  ylab("log response") + theme(legend.position="none") 
```

- Same trends as with first model.  

### Set 3: xa_08, xb_07, region

**First model**  

```{r mod_2B3.3, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Effect of nonlinear predictor `xa_08` depends on the `region`:
  - In region `XX`, its increase slightly decreases the log-response;
  - In region `YY`, its increase greatly decreases the log-response;
  - It does not have too much effect for region `ZZ`. 
- The nonlinear predictors have large model uncertainties (mean behavior), especially at extreme predictor values. 
  - This may suggest overfit when the inputs are at extremes. 
- Increasing the linear predictor `xb_07` slightly increase the response (horizontal facets).
  - Though the magnitude of the effect is overshadowed by the nonlinear predictor (the coefficients are small). 

**Second model**  

```{r mod_2B1.3, out.width=800, fig.width=8, fig.asp=0.6, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xa_08", "xb_07"), "region")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xa_08, fill=xb_07)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xb_07) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xb_07", breaks = NULL, labels = NULL)) + 
  scale_fill_viridis_b() + ylab("log response")
```

- Compared to the first model with `region` interacting with nonlinear features, this linear additive only model is less flexible and unable to adapt to the trend based on `region` because there is no interaction. 
  - All we get is that region shifts the trend up and down. 
- This resulted higher prediction uncertainty but relatively smaller model uncertainty as compared to the other model.  

### Set 4: xb_05, xw_01, region 

**First model**

```{r mod_2B3.4, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B3_2Ac3, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- Increasing nonlinear predictor `xb_05` has different effects based on `region`:
  - Slightly increase the log-response at regions `XX` and `ZZ`
  - Slightly decrease the log-response at region `YY`
- The nonlinear predictor `xw_01` increases response. 

**Second model**

```{r mod_2B1.4, out.width=800, fig.width=8, fig.asp=0.60, warning=FALSE}
viz_grid <- make_test_input_grid(my_inputs, df_preproc, c("xb_05", "xw_01"), "region")

make_post_pred(mod_2B1_2A3, viz_grid) %>% 
  ggplot(aes(x=xb_05, fill=xw_01)) +
  geom_ribbon(aes(ymin = y_lwr, ymax = y_upr), alpha=0.6) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.8, fill="grey") +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(region~xw_01) +
  scale_x_continuous(sec.axis = sec_axis(~ . , name = "xw_01", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_b() + ylab("log response")
```

- We see similar effect as with the prior comparisons, that the second model is unable to fit more closely to observations in the specific category, since we did not involve any interaction in this model. 

----

## Conclusions

- For the inputs that are visualized, whether predictive trends are consistent across the two models depends on what we examine.
- The trends from simple additive linear predictors and categorical predictors are almost identical. 

- For inputs that involve interaction:
  - The model with interaction from `region` to continuous inputs is more capable of explaining the observed variation per `region` category, as compared to the model with no interaction. 
  - But this may cause higher risk of overfitting, when the observations are rare: in our data, when the inputs are at extreme values. 

----

  
