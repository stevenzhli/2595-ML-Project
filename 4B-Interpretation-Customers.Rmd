---
title: "INFSCI 2595 Final Project"
subtitle: "Part 4B, Interpretation - Customers"
author: "Zhenyu Li"
date: '2022-04-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
library(vip)
source("./scripts/utils.R")
```


```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_regr <- df %>% select(-c(rowid,outcome)) # remove unused columns
df_clas <- df %>% select(-c(rowid,response)) # remove unused columns
```

----

Tasks

- Identify the customers appears to be hardest to predict for the regression and classification tasks
  - Base your conclusions on the best performing regression and classification models.
    - You should base your conclusions on the resampled hold-out sets and NOT on the training set!
    - Save your hold-out set predictions!

----

## Unpredictable Customer

The best model for the **regression task** was identified in part 2D4 as the XGBoost model. 
The best model for the **classification task** was identified in part 3D4 as the Elastic Net model. 

**Read in the respective best model for each task.**

```{r}
mod_rg_xgb <- readr::read_rds("./models/mod_2D_adv_best_wflow")
mod_rg_xgb_resample <- readr::read_rds("./models/mod_2D_adv_best_resample")

mod_cl_enet <- readr::read_rds("./models/mod_3D_enet_best_wflow")
mod_cl_enet_resample <- readr::read_rds("./models/mod_3D_enet_best_resample")
```


- To find the most unpredictable customer, need to visualize the prediction with regard to each customer. 
- The respective resampled predictions are saved in the model resample files, and will be used for model evaluation. 

----

## Regression

**Summarize the prediction error (MAE) across the resample repeated folds for each customer.** 

```{r mae, out.width=600, fig.width=6, fig.asp=0.4, message=FALSE, warning=FALSE}
regr_pred <- mod_rg_xgb_resample %>% 
  # get all predictions across folds and repeats
  collect_predictions() %>% 
  # join the training df
  left_join(rowid_to_column(select(df_regr,-response)), by = c(".row" = "rowid"))

regr_pred %>% 
  group_by(id, id2, customer) %>% 
  mae(response, .pred) %>% 
  group_by(customer) %>% 
  summarise(mae = mean(.estimate), mae_se = se(.estimate)) %>% 
  ggplot() +
  geom_pointrange(aes(
    x=customer,
    y=mae, 
    ymax=mae + mae_se, 
    ymin = mae - mae_se,
    ))
```

- The `response` from customer K is the most difficult to predict using my best regression model.  

----

## Classification

**Generate the ROC curve for each customer to visualize model performance.**   

```{r roc, out.width=600, fig.width=6, fig.asp=1, message=FALSE, warning=FALSE}
clas_pred <- mod_cl_enet_resample %>% 
  collect_predictions(parameters = mod_cl_enet$param) %>% 
  left_join(rowid_to_column(select(df_clas,-outcome)), by = c(".row" = "rowid"))    
  
clas_pred %>% 
  group_by(customer) %>% 
  roc_curve(outcome, .pred_event) %>% 
  autoplot() + facet_wrap(~ customer) + 
  theme(legend.position = "none")
```

- As mentioned earlier in 4D1, ROC is not an ideal metric since we have unbalanced events. This problem is exaggerated by splitting the observations further by `customer`. As demonstrated by the "perfect" ROC curve of customer E.  

**Summarize the mean log loss across the resample repeated folds for each customer.** 

```{r mll, out.width=600, fig.width=6, fig.asp=0.4, message=FALSE, warning=FALSE}
clas_pred %>% 
  group_by(id, id2, customer) %>% 
  mn_log_loss(outcome, .pred_event) %>% 
  group_by(customer) %>% 
  summarise(mn_log_loss = mean(.estimate), se = se(.estimate)) %>% 
  ggplot() +
  geom_pointrange(aes(
    x=customer,
    y=mn_log_loss, 
    ymax=mn_log_loss + se, 
    ymin = mn_log_loss - se,
    ))
```

- The `outcome` from customer `Other` appears to be the most difficult to predict using my best classification model. Followed by customer A and Q. 


----

