---
title: "INFSCI 2595 Final Project"
subtitle: "Part 2B, Regression - Bayesian Models"
author: "Zhenyu Li"
date: '2022-04-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(corrplot)
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
sel_num <- df %>% select_if(is_double) %>% select(!rowid) %>% select(sort(names(.))) %>% colnames()
sel_num_in <- setdiff(sel_num,"response")
sel_cat <- df %>% select(!one_of(sel_num)) %>% select(!rowid) %>% colnames()
sel_cat_in <- setdiff(sel_cat, "outcome")
```

----

In this section, I will use Bayesian approach to evaluate 3 baseline models with relatively good performance from section 2A.  

----

## Preprocessing

- For regression, the necessary preprocessing steps include:
  - Log-transform the continuous response
  - Normalize the continuous inputs

```{r}
df_preproc <- df %>% 
  mutate_if(is.character,as.factor) %>% 
  select(-c(rowid,outcome)) %>% 
  mutate(
    response = log(response),
    across(starts_with("x"), scale)
  )
```

----

## Bayesian Model Fitting

**Select 3 models from 2A for Bayesian model fitting.** 

- Model `mod_2A3` is best model according to BIC, which consist of only linear additive of linear inputs. 
- Model `mod_2Ac1` has intermediate AIC/BIC result, by adding some nonlinear features based on `mod_2A3`. 
- Model `mod_2Ac3` is best model according to AIC, by adding interaction from `region` to the added nonlinear features from `mod_2Ac1`. 
- I will use weak prior of R-squared 0.5. 

```{r}
mod_2A3 <- readr::read_rds("models/mod_2A3")
mod_2Ac1 <- readr::read_rds("models/mod_2Ac1")
mod_2Ac3 <- readr::read_rds("models/mod_2Ac3")
```

**1. Based on model `mod_2A3` (linear additive of all inputs).**

```{r model 2B1_2A3}
mod_2B1_2A3 <- stan_lm(
  mod_2A3$call$formula,
  data=df_preproc,
  prior = R2(location = 0.5),
  seed = 1324
)
```


**2. Based on model `mod_2Ac3`**

```{r model 2B2_2Ac1}
mod_2B2_2Ac1 <- stan_lm(
  mod_2Ac1$call$formula,
  data=df_preproc, 
  prior = R2(location = 0.5),
  seed = 1324
)
```


**3. Based on model `mod_2Ac1`.** 

```{r model 2B3_2Ac3}
mod_2B3_2Ac3 <- stan_lm(
  mod_2Ac3$call$formula,
  data=df_preproc, 
  prior = R2(location = 0.5),
  seed = 1324
)
```

**Save the 3 models to file.**  

```{r}
my_models <- mget(ls(pattern = "^mod_2B"))
save_models(my_models)
```

----

## Conclusions

```{r}
mod_2B1_2A3 <- readr::read_rds("models/mod_2B1_2A3")
mod_2B2_2Ac1 <- readr::read_rds("models/mod_2B2_2Ac1")
mod_2B3_2Ac3 <- readr::read_rds("models/mod_2B3_2Ac3")
```

### Model Comparison

**Compare models based on information criterion.**  

```{r}
mod_2B1_2A3$waic <- waic(mod_2B1_2A3)
mod_2B2_2Ac1$waic <- waic(mod_2B2_2Ac1)
mod_2B3_2Ac3$waic <- waic(mod_2B3_2Ac3)
```

```{r}
my_models <- stanreg_list(
  mod_2B1_2A3, mod_2B2_2Ac1, mod_2B3_2Ac3,
  model_names = c("2B1_2A3, all linear","2B2_2Ac1, add nonlinear","2B3_2Ac3, region: non-linear"))
loo_compare(my_models, criterion = "waic")
```

- I choose WAIC for model selection, since at this stage we use training set only for evaluation.  
- Based on WAIC, model `mod_2B3_2Ac3` with interaction from `region` to nonlinear features is the best model.
- For the other two models, the performance difference are both greater than the respective s.e. on the difference from the best model. 


### Best Model

**Analyze the posterior behavior of the best model.**  

#### R-squared

```{r}
bayes_R2(mod_2B3_2Ac3) %>% quantile(c(0.05, 0.5, 0.95))
```

- Data changed our prior belief about the model uncertainty (R-squared).  
  - Posterior lower 5th quantile at 0.65, while prior belief was 0.5 for R-squared.  

#### Coefficients

**Visualize posterior regression coefficient plot. **

```{r Uncertainty, out.width=400, fig.width=4, fig.asp=2}
plot(mod_2B3_2Ac3, pars = names(mod_2B3_2Ac3$coefficients)) +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 0.5)
```

**Check the correlation of the posterior coefficients.**  

```{r Correlations, out.width=500, fig.width=5, fig.asp=1, warning=FALSE}
mod_2B3_2Ac3 %>% as_tibble() %>% 
  select(all_of(names(mod_2B3_2Ac3$coefficients))) %>% 
  cor() %>% corrplot(method="color", type="upper")
```

- Some positive correlation between `region` interacting with the nonlinear features. 

#### Model Uncertainty

**Tasks  **

- Study the posterior uncertainty in the noise (residual error), ùúé.
- How does the `lm()` maximum likelihood estimate (MLE) on ùúé relate to the posterior uncertainty on ùúé ?
- Do you feel the posterior is precise or are we quite uncertain about ùúé

**Visualize and compare posterior distribution of œÉ across 3 models.**  

```{r}
my_models <- mget(ls(pattern = "^mod_2B"))
my_models %>% pull_bayes_posterior_sigma() %>% 
  ggplot(aes(x = sigma)) +
  geom_freqpoly(aes(color = model),bins = 50) +
  geom_vline(xintercept = sigma(mod_2Ac3), color="#4D9EFF", linetype="dotted", size=0.5) +
  geom_vline(xintercept = sigma(mod_2B3_2Ac3), color="#4D9EFF", linetype="dashed", size=0.5)
```


- Model `mod_2B3_2Ac3` has the lowest posterior œÉ mode (blue dashed line). 
  - The amount of uncertainty is similar for all three models, spanning the range of about 0.04. 
  - Thus the posterior œÉ is fairly precise as none spans a large range. 
  

**Compute the numbers on the best model.**

```{r}
sprintf("Posterior mode œÉ: %.3f", sigma(mod_2B3_2Ac3))
sprintf("MLE estimated œÉ: %.3f", sigma(mod_2Ac1))

my_mod_sigma <- mod_2B3_2Ac3 %>% as_tibble() %>% select(sigma)
sprintf("Probability of posterior œÉ > MLE œÉ: %.3f%%", mean(my_mod_sigma >= sigma(mod_2Ac3))*100)
```


- For the best model identified using WAIC `mod_2B3_2Ac3`:
  - The posterior mode of œÉ from the Bayesian model is around 0.299 (blue dashed line).
  - The corresponding MLE estimated of œÉ is 0.292 (blue dotted line).
  - The MLE underestimated œÉ: there is 94.2% chance œÉ is greater than the MLE estimate, judging from the posterior samples. 


----
