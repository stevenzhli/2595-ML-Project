---
title: "INFSCI 2595 Final Project"
subtitle: "Part 3C, Classification - Prediction"
author: "Zhenyu Li"
date: '2022-04-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(rstanarm)
options(mc.cores = parallel::detectCores())
library(bayesplot)
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_prep_cl <- df %>% 
  mutate_if(is.character,as.factor) %>% 
  select(-c(rowid,response)) %>% 
  mutate(
    across(starts_with("x"), scale)
  )
my_inputs <- setdiff(names(df_prep_cl),"outcome")
```

----

In this section, I will visualize how the generalized linear models predict the event probability of `outcome` based on some selected inputs.  

----

## Preparation

### Model Selection

**Select two Bayesian models from 3B for prediction and visualization.**   

- Since all three Bayesian models have similar performance, I will just choose the two with more features so that there are more potential features to select for visualization. 

- One with all inputs in linear additive relationship `mod_3B2_3A3`. 

- One with some interactions of the inputs `mod_3B3_3Ac2`. 

```{r}
mod_3B2_3A3 <- readr::read_rds("models/mod_3B2_3A3")
mod_3B3_3Ac2 <- readr::read_rds("models/mod_3B3_3Ac2")
```

### Input Selection

**Select inputs to visualize, based on model coefficient significance.**  

**First model,  plot of significant coefficients.**  

```{r mod_3B2_3A3}
mod_3B2_3A3 %>% as_tibble() %>% 
  select(all_of(
    mod_3A3 %>% pull_lm_coefs_signif(top_n = 20) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

**Second model, plot of significant coefficients.**  

```{r mod_3B3_3Ac2}
mod_3B3_3Ac2 %>% as_tibble() %>% 
  select(all_of(
    mod_3Ac2 %>% pull_lm_coefs_signif(top_n = 20) %>% select(coef) %>% unlist(use.names=F)
  )) %>% 
  # mcmc_areas_ridges(prob=0.6, prob_outer = 0.9) +
  mcmc_intervals() +
  geom_vline(xintercept = 0,linetype="dashed")
```

**Choose predictors to use based on both models. **  

- Selection criterion
  1. Since continuous inputs are normalized, the greater magnitude of the estimated coefficient indicates more impact from this input to the `response`.
  2. If two coefficients show significant difference on the coefplot they worth looking at. 
  3. Examine both continuous and categorical inputs. 

- Selected 3 sets of inputs for visualization
  1. xw_03, xn_07
  2. xw_03, xn_07, xn_08
  3. xw_03, customer 
  4. xw_02, xs_03

----

## Predictions

### Set 1: xw_03, xn_07 

This set of inputs are associated with the top 2 most significant coefficients.  

```{r}
viz_grid <- make_test_input_grid(my_inputs, df_prep_cl, c("xw_03", "xn_07"))
```

**First model**  

```{r mod_3B2.1, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
make_post_pred_cl(mod_3B2_3A3, viz_grid) %>% ggplot(aes(x=xw_03, fill=xn_07)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xn_07, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xn_07", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- The predicted mean event probability decreases with the input `xw_03`. (each facet)
- The predicted mean event probability increases with the input `xn_07`. (across facets)
- There is higher uncertainty in model behavior 
  - when the `xw_03` is high (each facet, right), or 
  - when the `xn_07` is low (across facets, left)

**Second model**  

```{r mod_3B3.1, out.width=800, fig.width=8, fig.asp=0.24, warning=FALSE}
make_post_pred_cl(mod_3B3_3Ac2, viz_grid) %>% ggplot(aes(x=xw_03, fill=xn_07)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xn_07, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xn_03", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- The second model shows consistent event probability trends as the first model.
- Though the model uncertainty is even higher when input `xn_03` is low. (wider purple range)
- But model uncertainty seems to improve when input `xn_03` is high. (thinner yellow range)


### Set 2: xw_03, xn_07, xn_08 

This set of inputs are associated with the top 3 most significant coefficients.  

```{r}
viz_grid <- make_test_input_grid(my_inputs, df_prep_cl, c("xw_03", "xn_07", "xn_08"))
```

**First model**  

```{r mod_3B2.2, out.width=800, fig.width=8, fig.asp=0.8, warning=FALSE}
make_post_pred_cl(mod_3B2_3A3, viz_grid) %>% ggplot(aes(x=xw_03, fill=xn_07)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(xn_08 ~ xn_07, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xn_07", breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ ., name = "xn_08", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- This basically expands our visualization on set 1 by the third significant continuous input `xn_08`. 
- We see the 3rd input `xn_08` is responsible for some of the uncertainty when `xw_03` is low (left-most vertical facets).
  - When both `xn_07` and `xn_08` are low, there are more uncertainty in the prediction (top-left facet). 
  - If either one increases, there are less uncertainty in the predictions.  

**Second model**  

```{r mod_3B3.2, out.width=800, fig.width=8, fig.asp=0.8, warning=FALSE}
make_post_pred_cl(mod_3B3_3Ac2, viz_grid) %>% ggplot(aes(x=xw_03, fill=xn_07)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(xn_08 ~ xn_07, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xn_07", breaks = NULL, labels = NULL)) +
  scale_y_continuous(sec.axis = sec_axis(~ ., name = "xn_08", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- Same trend as with the first model. Just more uncertainty with low values, but more precise when inputs have higher values. 

### Set 3: xw_03, customer

This set expands the most significant continuous input with the customer categorical input. 

```{r}
viz_grid <- make_test_input_grid(my_inputs, df_prep_cl, c("xw_03"), "customer")
```

**First model**  

```{r mod_3B2.3, out.width=800, fig.width=8, fig.asp=0.2, warning=FALSE}
make_post_pred_cl(mod_3B2_3A3, viz_grid) %>% ggplot(aes(x=xw_03, fill=customer)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer, scales="free_y") +
  scale_fill_viridis_d() + ylab("event prob")
```

- This plot aims to look at how the most significant continuous input can predict different customers. 
  - It is clear that `customer` category affects the ability for `xw_03` to predict. (horizontal facets)
  - It is especially uncertain to predict for customers A and M. 

**Second model**  

```{r mod_3B3.3, out.width=800, fig.width=8, fig.asp=0.2, warning=FALSE}
make_post_pred_cl(mod_3B3_3Ac2, viz_grid) %>% ggplot(aes(x=xw_03, fill=customer)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~customer, scales="free_y") +
  scale_fill_viridis_d() + ylab("event prob")
```

- The overall event probability trends are consistent across the two models. 
- The uncertainty to predict the aforementioned customers is higher, but slightly lower for the rest customers.  

### Set 4: xw_02, xs_03

This set aims to examine how the interactions affects the predictions, thus uses the inputs associated with the most significant coefficient with interactions.  

```{r}
viz_grid <- make_test_input_grid(my_inputs, df_prep_cl, c("xw_02","xs_03"))
```

**First model**  

```{r mod_3B2.4, out.width=800, fig.width=8, fig.asp=0.2, warning=FALSE}
make_post_pred_cl(mod_3B2_3A3, viz_grid) %>% ggplot(aes(x=xw_02, fill=xs_03)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xs_03, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xs_03", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- The first model does not have interaction features, thus the overall trend does not change w.r.t. the two inputs. 

**Second model**  

```{r mod_3B3.4, out.width=800, fig.width=8, fig.asp=0.2, warning=FALSE}
make_post_pred_cl(mod_3B3_3Ac2, viz_grid) %>% ggplot(aes(x=xw_02, fill=xs_03)) +
  geom_ribbon(aes(ymin = trend_lwr, ymax = trend_upr), alpha=0.6) +
  geom_line(aes(y = trend_avg), size=0.5) +
  facet_grid(~xs_03, scales="free_y") +
  scale_x_continuous(sec.axis = sec_axis(~ ., name = "xs_03", breaks = NULL, labels = NULL)) +
  scale_fill_viridis_c() + ylab("event prob")
```

- With the second model, we see the interaction does have some effects.
- The prediction of event probability by first term `xw_02` is affected by the value of the second term `xs_03`.  
  - When both inputs are lower (purple, left), there is higher uncertainty in prediction and predicts slightly lower event probability.  
  - When `xs_03` is higher (yellow), there is tremendous uncertainty for predicting event probability once `xw_02` value exceeds 0. But under low `xw_02` values the model predicts high event probability.
  
----

## Conclusions

- The overall predicted event probability trend is consistent across the two models compared.  
- We see some features could affect the uncertainty in model prediction. 
- It's worth consider adding the interactions between categorical inputs and the continuous inputs. But because of the way I crafted models (arbitrary interactions) and filtered models (information criterion), these possibilities were ruled out due to added model complexity. 
- I will consider using regularization to tune these complex models with interactions to find those interactive terms that make most sense. 

----