---
title: "INFSCI 2595 Final Project"
subtitle: "Part 2D2, Regression - Resampling - ENET"
author: "Zhenyu Li"
date: '2022-04-20'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_regr <- df %>% select(-c(rowid,outcome)) %>% mutate(response = log(response))
```

----

## Elastic Net

Use Elastic Net to provide regularization to the two complex models that resulted extreme poor performances during the resampling.  

### Models

**Define the linear models to tune with Elastic Net.**  

**The preprocessing steps**  

```{r}
bp_prep <- recipe(response ~ ., data=df_regr) %>%
  step_normalize(all_numeric_predictors())
  # prep(training=df_regr, retain=T) %>% bake(new_data=NULL)
```

**`mod_2D2`: All pairwise interactions of continuous inputs, include additive categorical features.**  

```{r mod_2D2}
bp_2D2 <- bp_prep %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), sep = ":") %>%
  step_dummy(all_nominal_predictors())
```

**`mod_2D5_2Ac4`: All pairwise interactions for all input variables.**  

```{r mod_2D5}
bp_2D5_2Ac4 <- bp_prep %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), sep=":") %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ starts_with("region"):matches("^x.{1}_\\d+$"), sep=":") %>%
  step_interact(~ starts_with("customer"):matches("^x.{1}_\\d+$"), sep=":") %>%
  step_interact(~ matches("^region_.{2}$"):matches("^customer_.{1,6}$"), sep=":")
```

----

## Tuning

### Explore

**First use Lasso to explore the range for parameter tuning.**  

**Define the Lasso only model specification.**  

```{r}
lasso_explore_spec <-
  linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet", intercept = TRUE)
```

**Fit both models with Lasso.**  

```{r}
lasso_fit_2D2 <- workflow() %>%
  add_model(lasso_explore_spec) %>%
  add_recipe(bp_2D2) %>%
  fit(df_regr)

lasso_fit_2D5 <- workflow() %>%
  add_model(lasso_explore_spec) %>%
  add_recipe(bp_2D5_2Ac4) %>%
  fit(df_regr)
```

**Generate the coefficient paths.**  

```{r}
lasso_fit_2D2 %>% extract_fit_parsnip() %>%
  pluck('fit') %>%
  plot(xvar = 'lambda')

lasso_fit_2D5 %>% extract_fit_parsnip() %>%
  pluck('fit') %>%
  plot(xvar = 'lambda')
```

### Setup

Allow parallel processing.  

```{r message=FALSE, warning=FALSE}
if (parallel::detectCores(logical=FALSE) > 3) {
  library(doParallel)
  num_cores <- parallel::detectCores(logical=FALSE)
  cl <- makePSOCKcluster(num_cores - 2)
  registerDoParallel(cl)
}
```

**Create the custom search grid for the penalty strength and mixing fraction parameters.**  

- Use the bound of penalty strength for the range of parameters I am interested to tune.  
- Use 0.1 as lower bound for mixing fraction to avoid fully Ridge.  

```{r tuning grid}
# set tuning parameter ranges
my_lambda <- penalty(range = c(-6,-1), trans = log_trans())
my_alpha <- mixture(range = c(0.1, 1.0))
# construct the tuning grid
enet_grid <- grid_regular(
  my_lambda, my_alpha,
  levels = c(penalty = 80, mixture = 5))
```

**Define the resampling scheme**  

- Use the 5-fold cross-validation with 5 repeats.
- Model specification: use `glmnet` engine.

```{r resampling}
set.seed(2543)
cv_folds <- vfold_cv(df_regr, v = 5, repeats = 5)
my_metrics <- metric_set(rmse, mae, rsq)
```

**Create workflow set.**  

```{r workflow}
enet_spec <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet", intercept = TRUE, standardize = TRUE)

enet_wset <-
  workflow_set(
    preproc = list(
      all_cont_pairwise = bp_2D2,
      all_pairwise = bp_2D5_2Ac4
      ),
    models = list(enet = enet_spec)
  )
```

### Execute

```{r message=FALSE, warning=FALSE}
tune_2D_enet <- enet_wset %>%
  workflow_map(
    fn = "tune_grid",
    grid = enet_grid,
    resamples = cv_folds,
    metrics = my_metrics,
    control = control_grid(
      # save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = T
  )
```

```{r save result}
readr::write_rds(tune_2D_enet,"./models/tune_2D_enet")
```

----

## Conclusions

```{r}
tune_2D_enet <- readr::read_rds("./models/tune_2D_enet")
```

### Performance

**Visualize the model performance metrics across the two models tuned.** 

```{r visualize, out.width=800, fig.width=8, fig.asp=1, message=FALSE, warning=FALSE}
names(tune_2D_enet$result) <- tune_2D_enet$wflow_id

tune_2D_enet$result %>%
  map_dfr(collect_metrics, .id="wflow_id") %>%
  ggplot(aes(x = log(penalty))) +
  geom_ribbon(
    aes(ymin = mean - std_err, ymax = mean + std_err,
        group = interaction(mixture, .metric),
        fill = as.factor(mixture)),
    alpha = 0.3) +
  geom_line(
    aes(y = mean,
        group = interaction(mixture, .metric),
        color = as.factor(mixture)),
    size = 1) +
  facet_grid(.metric ~ wflow_id, scales = "free_y") +
  xlab("log-penalty") + ylab("performance") + labs(color="mixture", fill="mixture") +
  scale_color_viridis_d() + scale_fill_viridis_d()
```

- Elastic Net prefers Ridge over Lasso (small mixture fraction). This is as expected because some inputs are correlated, which can make Lasso struggle.  
- The model performances from "all pairwise interactions" (right side grids) have overall better performances compared to the "continuous pairwise interactions" (left side grids).  
- Judging from MAE (top grid row) the best log-penalty is at about -3.  

### Best Model

**Select the best tuning parameters based on the one-standard-error rule.**  

```{r}
enet_best_mae_params <-
  tune_2D_enet$result %>%
  map_dfr(select_by_one_std_err,
            desc(penalty), desc(mixture), metric = 'mae',
          .id = "wflow_id")
enet_best_mae_params %>% select(-starts_with("."))
```

- Comparing across the best of the two, the "all pairwise interactions" one is clear winner.  

**Finalize the workflow for the best model and retrain with resampling to save predictions.** 

```{r}
enet_best_wflow_mae <- "all_pairwise_enet"

mod_2D_enet_best_wflow <-
  enet_wset %>%
  extract_workflow(enet_best_wflow_mae) %>%
  finalize_workflow(
    parameters = enet_best_mae_params %>%
      filter(wflow_id==enet_best_wflow_mae) %>%
      select(all_of(names(enet_grid)))
  )

mod_2D_enet_best_resample <-
  mod_2D_enet_best_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = my_metrics,
    control = control_resamples(save_pred = T)
  )

mod_2D_enet_best_resample %>% collect_metrics() %>% select(-.config, -.estimator)
```

```{r}
save_models(mget(ls(pattern = "^mod_2D_enet")))
```

----
