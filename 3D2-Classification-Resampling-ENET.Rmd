---
title: "INFSCI 2595 Final Project"
subtitle: "Part 3D1, Classification - Resampling - ENET"
author: "Zhenyu Li"
date: '2022-04-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_clas <- df %>% select(-c(rowid,response)) # remove unused columns
```


## Elastic Net

Use Elastic Net to provide regularization to the two complex models that resulted extreme poor performances during the resampling.  

### Models

**Define the linear models to tune with Elastic Net.**  

```{r preprocess}
bp_prep_cl <- recipe(outcome ~ ., data=df_clas) %>%
  step_normalize(all_numeric_predictors())
  #prep(training=df_clas, retain=T) %>% bake(new_data=NULL)
```


- `mod_3D2`: All pairwise interactions of continuous inputs, include additive categorical features.
  
```{r mod_3D2}
bp_3D2 <- bp_prep_cl %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), sep = ":") %>%
  step_dummy(all_nominal_predictors())
```

- `mod_3D6_3A5`: interact `customer` with continuous inputs.  

```{r mod_3D6_3A5}
bp_3D6_3A5 <- bp_prep_cl %>%
  step_dummy(customer, one_hot = T) %>%
  step_interact(~ starts_with("customer_") : starts_with("x"), sep = ":") %>%
  step_dummy(region)
```

----

## Tuning

### Explore

**First use Lasso to explore the range for parameter tuning.**

Define the Lasso only model specification.  

```{r}
lasso_explore_spec_cl <-
  logistic_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")
```

Fit both models with Lasso.

```{r}
lasso_fit_3D2 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D2) %>%
  fit(df_clas)

lasso_fit_3D6 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D6_3A5) %>%
  fit(df_clas)
```

Generate the coefficient paths.

```{r}
lasso_fit_3D2 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
lasso_fit_3D6 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
```
### Setup

**Allow parallel processing.**  

```{r message=FALSE, warning=FALSE}
if (parallel::detectCores(logical=FALSE) > 3) {
  library(doParallel)
  num_cores <- parallel::detectCores(logical=FALSE)
  cl <- makePSOCKcluster(num_cores - 2)
  registerDoParallel(cl)
}
```

**Create the custom search grid for the penalty strength and mixing fraction parameters.**

- Use the bound of penalty strength for the range of parameters I am interested to tune.
- Use 0.1 as lower bound for mixing fraction to avoid fully Ridge.  

```{r tuning grid}
# set tuning parameter ranges
my_lambda <- penalty(range = c(-8,-2), trans = log_trans())
my_alpha <- mixture(range = c(0.1, 1.0))
# construct the tuning grid
enet_grid_cl <- grid_regular(
  my_lambda, my_alpha,
  levels = c(penalty = 60, mixture = 5))
```

**Define the resampling scheme**

- Use the 5-fold cross-validation with 5 repeats.
- Model specification: use `glmnet` engine.

```{r resampling}
set.seed(1493)
enet_folds <- vfold_cv(df_clas, v = 5, repeats = 5)
my_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)
```

**Create workflow set.**

```{r workflow}
enet_spec_cl <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")

enet_wset_cl <-
  workflow_set(
    preproc = list(
      all_cont_pairwise = bp_3D2,
      inter_customer_cont = bp_3D6_3A5
      ),
    models = list(enet = enet_spec_cl)
  )
```

### Execute

```{r message=FALSE, warning=FALSE}
tune_3D_enet <- enet_wset_cl %>%
  workflow_map(
    fn = "tune_grid",
    grid = enet_grid_cl,
    resamples = enet_folds,
    metrics = my_metrics,
    control = control_grid(
      # save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = T
  )
```

```{r save result}
readr::write_rds(tune_3D_enet,"./models/tune_3D_enet")
```

----

## Conclusions

```{r}
tune_3D_enet <- readr::read_rds("./models/tune_3D_enet")
```

### Performance

**Visualize the model performance metrics across the two models tuned.** 

```{r visualize tuning, out.width=800, fig.width=8, fig.asp=.6, message=FALSE, warning=FALSE}
names(tune_3D_enet$result) <- tune_3D_enet$wflow_id

tune_3D_enet$result %>%
  map_dfr(collect_metrics, .id="wflow_id") %>%
  ggplot(aes(x = log(penalty))) +
  geom_ribbon(
    aes(ymin = mean - std_err, ymax = mean + std_err,
        group = interaction(mixture, .metric),
        fill = as.factor(mixture)),
    alpha = 0.2) +
  geom_line(
    aes(y = mean,
        group = interaction(mixture, .metric),
        color = as.factor(mixture)),
    size = 1) +
  facet_grid(.metric ~ wflow_id, scales = "free_y") +
  xlab("log-penalty") + ylab("performance") + labs(color="mixture", fill="mixture") +
  scale_color_viridis_d() + scale_fill_viridis_d()
```


### Best Model

**Select the best tuning parameters based on the one-standard-error rule from the mean log loss.**  


```{r}
enet_best_roc_params <-
  tune_3D_enet$result %>%
  map_dfr(select_by_one_std_err,
          desc(penalty), desc(mixture), metric = "mn_log_loss",
          .id = "wflow_id")
enet_best_roc_params %>% select(-starts_with("."))
```

- For the more complex model with pairwise interaction of all continuous inputs, a higher mixing rate is preferred, indicating more Lasso to shut off features.  
- The model with interaction from `customer` resulted better mean log loss performance. Thus will serve as best model here.  

**Finalize the workflow for the best model and retrain with resampling to save predictions.** 

```{r}
enet_best_wflow_roc <- "inter_customer_cont_enet"
enet_best_param <- enet_best_roc_params %>%
      filter(wflow_id==enet_best_wflow_roc) %>%
      select(all_of(names(enet_grid_cl)))

mod_3D_enet_best_wflow <-
  enet_wset_cl %>%
  extract_workflow(enet_best_wflow_roc) %>%
  finalize_workflow(parameters = enet_best_param)
mod_3D_enet_best_wflow$param <- enet_best_param

mod_3D_enet_best_resample <-
  mod_3D_enet_best_wflow %>%
  fit_resamples(
    resamples = enet_folds,
    metrics = my_metrics,
    control = control_resamples(save_pred = T)
  )

mod_3D_enet_best_resample %>% collect_metrics() %>% select(-.config, -.estimator)
```

```{r}
save_models(mget(ls(pattern = "^mod_3D_enet")))
```

----