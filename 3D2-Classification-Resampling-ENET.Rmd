---
title: "INFSCI 2595 Final Project"
subtitle: "Part 3D1, Classification - Resampling - ENET"
author: "Zhenyu Li"
date: '2022-04-24'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_linedraw())
library(tidymodels)
tidymodels_prefer()
source("./scripts/utils.R")
```

```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
df_clas <- df %>% select(-c(rowid,response)) # remove unused columns
```


## Elastic Net

Use Elastic Net to provide regularization to the two complex logistic models that resulted poor performances during the resampling.  

### Models

**Define the linear models to tune with Elastic Net.**  

```{r preprocess}
bp_prep_cl <- recipe(outcome ~ ., data=df_clas) %>%
  step_normalize(all_numeric_predictors())
  #prep(training=df_clas, retain=T) %>% bake(new_data=NULL)
```

**`mod_3D2`: All pairwise interactions of continuous inputs, include additive categorical features.**  
  
```{r mod_3D2}
bp_3D2 <- bp_prep_cl %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), sep = ":") %>%
  step_dummy(all_nominal_predictors())
```

**`mod_3D6_3A5`: interact `customer` with all continuous inputs.**  

- This is modified from the `mod_3D5` where `customer` interacts with some selected continuous inputs. Because we are using regularization, I included more features here.  

```{r mod_3D6_3A5}
bp_3D6_3A5 <- bp_prep_cl %>%
  step_dummy(customer, one_hot = T) %>%
  step_interact(~ starts_with("customer_") : starts_with("x"), sep = ":") %>%
  step_dummy(region)
```

----

## Tuning

### Explore

**First use Lasso to explore the range for parameter tuning.**  

Define the Lasso only model specification.  

```{r}
lasso_explore_spec_cl <-
  logistic_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")
```

Fit both models with Lasso.

```{r}
lasso_fit_3D2 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D2) %>%
  fit(df_clas)

lasso_fit_3D6 <- workflow() %>%
  add_model(lasso_explore_spec_cl) %>%
  add_recipe(bp_3D6_3A5) %>%
  fit(df_clas)
```

Generate the coefficient paths.

```{r}
lasso_fit_3D2 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
lasso_fit_3D6 %>% extract_fit_parsnip() %>%
  pluck("fit") %>%
  plot(xvar = "lambda")
```
### Setup

**Create the custom search grid for the penalty strength and mixing fraction parameters.**  

- Use the bound of penalty strength for the range of parameters I am interested to tune.
- Use 0.1 as lower bound for mixing fraction to avoid fully Ridge.  

```{r tuning grid}
# set tuning parameter ranges
my_lambda <- penalty(range = c(-8,-1), trans = log_trans())
my_alpha <- mixture(range = c(0.1, 1.0))
# construct the tuning grid
enet_grid_cl <- grid_regular(
  my_lambda, my_alpha,
  levels = c(penalty = 60, mixture = 5))
```

**Define the resampling scheme**  

- Use the 5-fold cross-validation with 5 repeats.
- Model specification: use `glmnet` engine.

```{r resampling}
set.seed(1324)
cv_folds <- vfold_cv(df_clas, v = 5, repeats = 5)
my_metrics_cl <- metric_set(accuracy, roc_auc, mn_log_loss)
```

**Create workflow set.**  

```{r workflow}
enet_spec_cl <-
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet", intercept = T, standardize = F, family = "binomial") %>%
  set_mode("classification")

enet_wset_cl <-
  workflow_set(
    preproc = list(
      all_cont_pairwise = bp_3D2,
      inter_customer_cont = bp_3D6_3A5
      ),
    models = list(enet = enet_spec_cl)
  )
```

### Execute

Note: to speed up knit, these two code blocks are set `eval=FALSE`. They can be manually executed in the rmarkdown file.  

```{r execute, eval=FALSE}
# create cluster to allow parallel computing
if (parallel::detectCores(logical=FALSE) > 3) {
  library(doParallel)
  num_cores <- parallel::detectCores(logical=FALSE)
  cl <- makePSOCKcluster(num_cores - 2)
  registerDoParallel(cl)
}
# start tuning
tune_3D_enet <- enet_wset_cl %>%
  workflow_map(
    fn = "tune_grid",
    grid = enet_grid_cl,
    resamples = cv_folds,
    metrics = my_metrics_cl,
    control = control_grid(
      # save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
    ),
    verbose = T
  )
# close the cluster
stopCluster(cl)
registerDoSEQ()
```

```{r save_result, eval=FALSE}
readr::write_rds(tune_3D_enet,"./models/tune_3D_enet")
```

----

## Conclusions

```{r}
tune_3D_enet <- readr::read_rds("./models/tune_3D_enet")
```

### Performance

**Visualize the model performance metrics across the two models tuned.** 

```{r visualize tuning, out.width=800, fig.width=8, fig.asp=1, message=FALSE, warning=FALSE}
names(tune_3D_enet$result) <- tune_3D_enet$wflow_id

tune_3D_enet$result %>%
  map_dfr(collect_metrics, .id="wflow_id") %>%
  ggplot(aes(x = log(penalty))) +
  geom_ribbon(
    aes(ymin = mean - std_err, ymax = mean + std_err,
        group = interaction(mixture, .metric),
        fill = as.factor(mixture)),
    alpha = 0.2) +
  geom_line(
    aes(y = mean,
        group = interaction(mixture, .metric),
        color = as.factor(mixture)),
    size = 1) +
  facet_grid(.metric ~ wflow_id, scales = "free_y") +
  xlab("log-penalty") + ylab("performance") + labs(color="mixture", fill="mixture") +
  scale_color_viridis_d() + scale_fill_viridis_d()
```

- The performances are similar for the two models. 

### Best Model

**Select the best tuning parameters for each model based on mean log loss.**  

```{r}
enet_best_roc_params <-
  tune_3D_enet$result %>%
  map_dfr(select_by_one_std_err, desc(mixture), metric = "mn_log_loss", .id = "wflow_id")
enet_best_roc_params %>% select(-starts_with("."))
```

- For the model with pairwise interaction of all continuous inputs, a higher mixing rate is preferred, indicating more Lasso to shut off features.  

**Visualize all three performance metrics for the best models.**

```{r visualize grid, out.width=800, fig.width=8, fig.asp=0.4, message=FALSE, warning=FALSE}
tune_3D_enet %>% collect_metrics() %>% 
  inner_join(enet_best_roc_params %>% select(wflow_id, .config), by = c("wflow_id", ".config")) %>% 
  ggplot(aes(color = wflow_id, x = wflow_id)) +
  geom_pointrange(aes(y=mean, ymax=mean+std_err, ymin=mean-std_err)) +
  facet_wrap(~ .metric, scales = "free_y") +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  xlab(label = NULL) + ylab(label = NULL) 
```

- The model with interaction from `customer` has better performance. 

**Fit both best models and examine the number of non-zero coefficients.**  

```{r message=FALSE, warning=FALSE}
enet_wset_cl %>%
  extract_workflow("all_cont_pairwise_enet") %>%
  finalize_workflow(parameters = enet_best_roc_params %>% slice(1)) %>% 
  fit(df_clas) %>% 
  tidy() %>% filter(estimate != 0) %>% nrow()

enet_wset_cl %>%
    extract_workflow("inter_customer_cont_enet") %>%
    finalize_workflow(parameters = enet_best_roc_params %>% slice(2)) %>% 
  fit(df_clas) %>% 
  tidy() %>% filter(estimate != 0) %>% nrow()
```

- There are slightly more features from the better model.  
- Select the model with interaction from `customer` as the best model.  

**Finalize the workflow for the best model and retrain with resampling to save predictions.** 

```{r}
enet_best_wflow_roc <- "inter_customer_cont_enet"
enet_best_param <- enet_best_roc_params %>%
      filter(wflow_id==enet_best_wflow_roc) %>%
      select(all_of(names(enet_grid_cl)))

mod_3D_enet_best_wflow <-
  enet_wset_cl %>%
  extract_workflow(enet_best_wflow_roc) %>%
  finalize_workflow(parameters = enet_best_param)
mod_3D_enet_best_wflow$param <- enet_best_param

mod_3D_enet_best_resample <-
  mod_3D_enet_best_wflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = my_metrics_cl,
    control = control_resamples(save_pred = T)
  )

mod_3D_enet_best_resample %>% collect_metrics() %>% select(-.config, -.estimator)
```

```{r}
save_models(mget(ls(pattern = "^mod_3D_enet")))
```

----

