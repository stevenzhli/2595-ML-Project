---
title: "INFSCI 2595 Final Project"
subtitle: "Part 2A, Regression - Baseline Models"
author: "Zhenyu Li"
date: '2022-04-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(coefplot)
source("./scripts/utils.R")
theme_set(theme_linedraw())
```


```{r message=FALSE, warning=FALSE}
df <- readr::read_csv("./data/final_project_train.csv", col_names = TRUE)
sel_num <- df %>% select_if(is_double) %>% select(!rowid) %>% select(sort(names(.))) %>% colnames()
sel_num_in <- setdiff(sel_num,"response")
sel_cat <- df %>% select(!one_of(sel_num)) %>% select(!rowid) %>% colnames()
sel_cat_in <- setdiff(sel_cat, "outcome")
```

----

In this section, several baseline linear models are trained using `lm`.  

- Models are compared based on AIC/BIC.  
- Important features from the best models are discussed.  

----

## Preprocessing

- For regression, the necessary preprocessing steps include:   
  - Log-transform the continuous response  
  - Standardize continuous inputs  

```{r}
df_preproc <- df %>%
  mutate_if(is.character,as.factor) %>%
  select(-c(rowid,outcome)) %>%
  mutate(
    response = log(response),
    across(starts_with("x"), scale)
  )
```

----

## Model Fitting

**Create linear models to fit the data.**  

### Predefined Models

**Fit 6 predefined linear models.**  

1. Categorical variables only – linear additive  
2. Continuous variables only – linear additive  
3. All categorical and continuous variables – linear additive  
4. Interact `region` with continuous inputs, do not include `customer`  
5. Interact `customer` with continuous inputs, do not include `region`  
6. All pairwise interactions of continuous inputs, do not include categorical inputs  

Slice the data with certain inputs to facilitate model building.  

```{r}
df_cat <- df_preproc %>% select(-all_of(sel_num_in))
df_num <- df_preproc %>% select(-all_of(sel_cat_in))
```

Fit models by providing formula.  

```{r}
# fit the predefined models
mod_2A1 <- lm(response ~ ., df_cat)
mod_2A2 <- lm(response ~ ., df_num)
mod_2A3 <- lm(response ~ ., df_preproc)
mod_2A4 <- lm(response ~ (region) * (. -customer), df_preproc)
mod_2A5 <- lm(response ~ (customer) * (. -region), df_preproc)
mod_2A6 <- lm(response ~ (.)^2, df_num)
```

Get a quick comparison of models using information criterion.  

```{r}
my_models <- mget(ls(pattern = "^mod_2A\\d+"))
calc_models_ic(my_models)
```

- `mod_2A3`: Linear additive relationship of all continuous and categorical inputs has the best BIC.  
- `mod_2A4`: Adding interaction from `region` seems to improve AIC at the cost of BIC.  

### Custom Basis Models

**Fit 4 custom basis models.**  

- Input features that involve some non-linearity from EDA are
  - `xa, xb, xn` at index `03, 05, 08`
  - `xs` index `01, 04, 06`
  - `xw` index `01, 02, 03`

**1. Add non-linearity using 2 dof natural spline to these inputs, keep linear for the rest inputs. Linear additive only.** 

```{r model 2Ac1}
mod_2Ac1 <- lm(
  response ~ . +
    + splines::ns(xa_03,df=2) + splines::ns(xa_05,df=2) + splines::ns(xa_08,df=2)
    + splines::ns(xb_03,df=2) + splines::ns(xb_05,df=2) + splines::ns(xb_08,df=2)
    + splines::ns(xn_03,df=2) + splines::ns(xn_05,df=2) + splines::ns(xn_08,df=2)
    + splines::ns(xs_01,df=2) + splines::ns(xs_04,df=2) + splines::ns(xs_06,df=2)
    + splines::ns(xw_01,df=2) + splines::ns(xw_02,df=2) + splines::ns(xw_03,df=2)
    - xa_03 - xa_05 - xa_08
    - xb_03 - xb_05 - xb_08
    - xn_03 - xn_05 - xn_08
    - xs_01 - xs_04 - xs_06
    - xw_01 - xw_02 - xw_03,
  data=df_preproc)
calc_models_ic(list(mod_2Ac1))
```

**2. Remove categorical inputs, keep the rest as 2Ac1.**  

```{r model 2Ac2}
mod_2Ac2 <- lm(
    response ~ . - region - customer
    + splines::ns(xa_03,df=2) + splines::ns(xa_05,df=2) + splines::ns(xa_08,df=2)
    + splines::ns(xb_03,df=2) + splines::ns(xb_05,df=2) + splines::ns(xb_08,df=2)
    + splines::ns(xn_03,df=2) + splines::ns(xn_05,df=2) + splines::ns(xn_08,df=2)
    + splines::ns(xs_01,df=2) + splines::ns(xs_04,df=2) + splines::ns(xs_06,df=2)
    + splines::ns(xw_01,df=2) + splines::ns(xw_02,df=2) + splines::ns(xw_03,df=2)
    - xa_03 - xa_05 - xa_08
    - xb_03 - xb_05 - xb_08
    - xn_03 - xn_05 - xn_08
    - xs_01 - xs_04 - xs_06
    - xw_01 - xw_02 - xw_03,
  df_preproc)
calc_models_ic(list(mod_2Ac2))
```

**3. Interact `region` with the nonlinear features from 2Ac1, additive for the rest.**  

```{r model 2Ac3}
mod_2Ac3 <- lm(
  response ~ . - region + region :
    ( splines::ns(xa_03,df=2) + splines::ns(xa_05,df=2) + splines::ns(xa_08,df=2)
    + splines::ns(xb_03,df=2) + splines::ns(xb_05,df=2) + splines::ns(xb_08,df=2)
    + splines::ns(xn_03,df=2) + splines::ns(xn_05,df=2) + splines::ns(xn_08,df=2)
    + splines::ns(xs_01,df=2) + splines::ns(xs_04,df=2) + splines::ns(xs_06,df=2)
    + splines::ns(xw_01,df=2) + splines::ns(xw_02,df=2) + splines::ns(xw_03,df=2) )
    - xa_03 - xa_05 - xa_08
    - xb_03 - xb_05 - xb_08
    - xn_03 - xn_05 - xn_08
    - xs_01 - xs_04 - xs_06
    - xw_01 - xw_02 - xw_03,
  df_preproc)
calc_models_ic(list(mod_2Ac3))
```

**4. Use pair-wise interaction for all input variables.**  

```{r model 2Ac4}
mod_2Ac4 <- lm(response ~ (.)^2, df_preproc)
calc_models_ic(list(mod_2Ac4))
```

Save the models to file.  

```{r}
my_models <- mget(ls(pattern = "^mod_2A"))
save_models(my_models)
```

----

## Conclusions

```{r}
mod_2A1 <- readr::read_rds("models/mod_2A1")
mod_2A2 <- readr::read_rds("models/mod_2A2")
mod_2A3 <- readr::read_rds("models/mod_2A3")
mod_2A4 <- readr::read_rds("models/mod_2A4")
mod_2A5 <- readr::read_rds("models/mod_2A5")
mod_2A6 <- readr::read_rds("models/mod_2A6")
mod_2Ac1 <- readr::read_rds("models/mod_2Ac1")
mod_2Ac2 <- readr::read_rds("models/mod_2Ac2")
mod_2Ac3 <- readr::read_rds("models/mod_2Ac3")
mod_2Ac4 <- readr::read_rds("models/mod_2Ac4")
```

### Model Comparison

**Print out the model metrics.**  

```{r}
my_models <- mget(ls(pattern = "^mod_2A"))
calc_models_ic(my_models) %>% knitr::kable()
```

- Use information criterion (AIC/BIC) for model selection since models are developed from training set without resampling.

- Special case
  - The all-pairwise model (`mod_2Ac4`) resulted large negative AIC/BIC. I talked to Dr. Yurko, and he suggests skipping this one for the Bayesian analysis because it will result enormous uncertainty. I will include for regularization in part 2D.
  - I will skip it here too, and examine only models with second lowest AIC, second lowest BIC, and one in between for both metrics.

- Best models
  - Based on the BIC, select `mod_2A3`.
  - Based on the AIC, select `mod_2Ac3`.
  - Based on balance of the two, select `mod_2Ac1`

### Coefficient Plot

**Visualize the coefficient summaries for the top 3 models.**  

```{r out.width=500, fig.width=5, fig.asp=1}
coefplot(mod_2A3)
coefplot(mod_2Ac1)
coefplot(mod_2Ac3)
```

- Each model contains some significant coefficients, but it looks like majority of them are non-significant.
- For the linear feature additive only model (`mod_2A3`) the coefficients are quite small, within 1.  
- For the two custom basis model with the nonlinear features coefficients are larger.  
- One interesting observation is that adding nonlinear features (mod_2Ac1) seem to have caused the `region` coefficients to become more uncertain from the all linear additive model (mod_2A3).  

### Important Features

**Extract significant coefficients and arrange by overall score.**  

```{r}
mod_2A3 %>% pull_lm_coefs_signif() %>% knitr::kable()
mod_2Ac1 %>% pull_lm_coefs_signif() %>% knitr::kable()
mod_2Ac3 %>% pull_lm_coefs_signif() %>% knitr::kable()
```

- For continuous inputs, potential important ones are `xw_01`, `xb_04`, `xb_07`, `xa_02`.
- For categorical inputs, `region` appears important, and some levels of `customer` too.
- Once we interact `region` to the nonlinear features, it is notable that interaction from `region` is important.

----
